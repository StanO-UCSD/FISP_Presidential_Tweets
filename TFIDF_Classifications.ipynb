{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pylab as plt\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## functions for lexical analysis\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens,stemmer)\n",
    "    return stems\n",
    "\n",
    "def text2vec(full_text, use_stemmer=False):\n",
    "    \"\"\"\n",
    "    Convert text to vectors using TFIDF\n",
    "    ngram_range=(1,3) means unigrams, bigrams and trigrams; \n",
    "    if want to use bigrams only, define ngram_range=(2,2)\n",
    "    \n",
    "    \"\"\"\n",
    "    text=full_text[:]\n",
    "    if use_stemmer:\n",
    "        vectorizer = TfidfVectorizer(tokenizer=tokenize,ngram_range=(1,3))\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "    text_vector = vectorizer.fit_transform(text)\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sys' has no attribute 'setdefaultencoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ee0ccf460898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefaultencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sys' has no attribute 'setdefaultencoding'"
     ]
    }
   ],
   "source": [
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the Data\n",
    "Load \"human-coded-tweets\" into a pandas dataframe. Data preprocessing (convert to lower case, remove punctuation, remove screen names. etc) should be done before this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "data = pd.DataFrame.from_csv('training_data.csv')\n",
    "\n",
    "# take the 'text' column from the dataframe, and \n",
    "# convert the text to vectors\n",
    "full_text = data['text'].astype('U')\n",
    "text_vec = text2vec(full_text, use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7638\n"
     ]
    }
   ],
   "source": [
    "##Find the number of rows in the training data, reserve 725 test tweets\n",
    "nrows = len(full_text)\n",
    "train_end= nrows-725\n",
    "print(train_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Split the data\n",
    "Split the data into a training set and a test set. Ideally the data should be shuffled before the split to avoid implicit bias in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train = text_vec[0:train_end,] # 7638 training tweets as of 2-21-2018\n",
    "pred_test = text_vec[train_end:,] # 725 test tweets\n",
    "pred_matrix = np.zeros((725,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Pick the right models\n",
    "The classification pipeline is: fitting the model with the training set -> predict labels on the test set -> compare predicted labels to real labels(human-coded-labels). These are all done on the human-coded-tweets, for the purpose of finding the best classification model with appropriate hyper-parameter settings. \n",
    "\n",
    "These could be done in one big for-loop, but it takes a long time to run, and it did crash my computer several times. What I did was to run classifications on only a few categories at a time - there would be many repeated code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment (Multinomial Naive Bayes); different alpha values would yield slightly different classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "==================================================\n",
      "Training Accuracy: 0.9886\n",
      "Test Accuracy: 0.6566\n",
      "Kappa: 0.4463\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.70      0.74      0.72       235\n",
      "          0       0.43      0.16      0.23       174\n",
      "          1       0.67      0.86      0.75       316\n",
      "\n",
      "avg / total       0.62      0.66      0.62       725\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = 'Sentiment'\n",
    "alpha = 0.4\n",
    "\n",
    "# real labels for the training set\n",
    "tar_train = data[key][0:train_end,] \n",
    "# real labels for the test set\n",
    "tar_test = data[key][train_end:,]\n",
    "\n",
    "# specify the classification model\n",
    "clf = MultinomialNB(alpha=alpha, fit_prior=True, class_prior=None)\n",
    "# fit the model with the training set\n",
    "clf.fit(pred_train, tar_train)\n",
    "# compute training accuracy\n",
    "train_score = clf.score(pred_train, tar_train)\n",
    "# predict labels on the test set\n",
    "y_pred = clf.predict(pred_test)\n",
    "\n",
    "# compute standard metrics\n",
    "test_accuracy = metrics.accuracy_score(tar_test.values.astype(int), y_pred)\n",
    "class_report = metrics.classification_report(tar_test.values.astype(int), y_pred)\n",
    "kappa = metrics.cohen_kappa_score(tar_test.values.astype(int), y_pred)\n",
    "\n",
    "print(key)\n",
    "print('='*50)\n",
    "print('Training Accuracy: '+'{:.4f}'.format(train_score))\n",
    "print('Test Accuracy: '+'{:.4f}'.format(test_accuracy))\n",
    "print('Kappa: '+'{:.4f}'.format(kappa))\n",
    "print(class_report)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Political & Makes_a_Factual_or_Verifiable_Claim (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Political\n",
      "==================================================\n",
      "Training Accuracy: 0.9992\n",
      "Test Accuracy: 0.8386\n",
      "Kappa: 0.1023\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.07      0.13       121\n",
      "          1       0.84      0.99      0.91       604\n",
      "\n",
      "avg / total       0.81      0.84      0.78       725\n",
      "\n",
      "\n",
      "\n",
      "Makes_a_Factual_or_Verifiable_Claim\n",
      "==================================================\n",
      "Training Accuracy: 0.9957\n",
      "Test Accuracy: 0.7159\n",
      "Kappa: 0.2240\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.93      0.82       494\n",
      "          1       0.63      0.26      0.37       231\n",
      "\n",
      "avg / total       0.70      0.72      0.67       725\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys = ['Political', 'Makes_a_Factual_or_Verifiable_Claim']\n",
    "\n",
    "for key in keys: \n",
    "    # real labels for the training set\n",
    "    tar_train = data[key][0:train_end,] \n",
    "    # real labels for the test set\n",
    "    tar_test = data[key][train_end:,]\n",
    "\n",
    "    # specify the classification model\n",
    "    clf = LinearSVC(class_weight='balanced')\n",
    "    # fit the model with the training set\n",
    "    clf.fit(pred_train, tar_train)\n",
    "    # compute training accuracy\n",
    "    train_score = clf.score(pred_train, tar_train)\n",
    "    # predict labels on the test set\n",
    "    y_pred = clf.predict(pred_test)\n",
    "\n",
    "    # compute standard metrics\n",
    "    test_accuracy = metrics.accuracy_score(tar_test.values.astype(int), y_pred)\n",
    "    class_report = metrics.classification_report(tar_test.values.astype(int), y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(tar_test.values.astype(int), y_pred)\n",
    "\n",
    "    print(key)\n",
    "    print('='*50)\n",
    "    print('Training Accuracy: '+'{:.4f}'.format(train_score))\n",
    "    print('Test Accuracy: '+'{:.4f}'.format(test_accuracy))\n",
    "    print('Kappa: '+'{:.4f}'.format(kappa))\n",
    "    print(class_report)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ideology & Immigration & Macroeconomic & National_Security & \n",
    "Crime & Civil_Rights & Environment & Education & Health_Care (Bagging Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideology\n",
      "==================================================\n",
      "Training Accuracy: 0.9967\n",
      "Test Accuracy: 0.5628\n",
      "Kappa: 0.1703\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.13      0.15      0.14        20\n",
      "          0       0.55      0.76      0.64       355\n",
      "          1       0.64      0.39      0.48       350\n",
      "\n",
      "avg / total       0.58      0.56      0.55       725\n",
      "\n",
      "\n",
      "\n",
      "Immigration\n",
      "==================================================\n",
      "Training Accuracy: 0.9988\n",
      "Test Accuracy: 0.9724\n",
      "Kappa: 0.0885\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99       704\n",
      "          1       1.00      0.05      0.09        21\n",
      "\n",
      "avg / total       0.97      0.97      0.96       725\n",
      "\n",
      "\n",
      "\n",
      "Macroeconomic\n",
      "==================================================\n",
      "Training Accuracy: 0.9980\n",
      "Test Accuracy: 0.8759\n",
      "Kappa: 0.2184\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.94      0.93       656\n",
      "          1       0.32      0.26      0.29        69\n",
      "\n",
      "avg / total       0.87      0.88      0.87       725\n",
      "\n",
      "\n",
      "\n",
      "National_Security\n",
      "==================================================\n",
      "Training Accuracy: 0.9992\n",
      "Test Accuracy: 0.8524\n",
      "Kappa: 0.0973\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.99      0.92       617\n",
      "          1       0.53      0.07      0.13       108\n",
      "\n",
      "avg / total       0.81      0.85      0.80       725\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hng/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime\n",
      "==================================================\n",
      "Training Accuracy: 0.9997\n",
      "Test Accuracy: 0.9834\n",
      "Kappa: 0.0000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       713\n",
      "          1       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.97      0.98      0.98       725\n",
      "\n",
      "\n",
      "\n",
      "Civil_Rights\n",
      "==================================================\n",
      "Training Accuracy: 0.9984\n",
      "Test Accuracy: 0.9352\n",
      "Kappa: -0.0205\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.95      0.97       716\n",
      "          1       0.00      0.00      0.00         9\n",
      "\n",
      "avg / total       0.97      0.94      0.95       725\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hng/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\n",
      "==================================================\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy: 0.9834\n",
      "Kappa: 0.0000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       713\n",
      "          1       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.97      0.98      0.98       725\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "==================================================\n",
      "Training Accuracy: 0.9996\n",
      "Test Accuracy: 0.9931\n",
      "Kappa: 0.0000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00       720\n",
      "          1       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.99      0.99      0.99       725\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hng/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "max_samples=0.7\n",
    "max_features=0.8\n",
    "keys = ['Ideology', 'Immigration', 'Macroeconomic', 'National_Security', 'Crime', 'Civil_Rights',\n",
    "        'Environment', 'Education']\n",
    "\n",
    "for key in keys: \n",
    "    # real labels for the training set\n",
    "    tar_train = data[key][0:train_end,] \n",
    "    # real labels for the test set\n",
    "    tar_test = data[key][train_end:,]\n",
    "\n",
    "    # specify the classification model\n",
    "    clf = BaggingClassifier(LinearSVC(class_weight='balanced'), \n",
    "                            max_samples=max_samples, max_features=max_features)\n",
    "    # fit the model with the training set\n",
    "    clf.fit(pred_train, tar_train) \n",
    "    # compute training accuracy\n",
    "    train_score = clf.score(pred_train, tar_train)\n",
    "    # predict labels on the test set\n",
    "    y_pred = clf.predict(pred_test)\n",
    "\n",
    "    # compute standard metrics\n",
    "    test_accuracy = metrics.accuracy_score(tar_test.values.astype(int), y_pred)\n",
    "    class_report = metrics.classification_report(tar_test.values.astype(int), y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(tar_test.values.astype(int), y_pred)\n",
    "\n",
    "    print(key)\n",
    "    print('='*50)\n",
    "    print('Training Accuracy: '+'{:.4f}'.format(train_score))\n",
    "    print('Test Accuracy: '+'{:.4f}'.format(test_accuracy))\n",
    "    print('Kappa: '+'{:.4f}'.format(kappa))\n",
    "    print(class_report)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Governance & No_Policy_Content & Asks_for_Donation &  Asks_you_to_watch_something_share_something_follow_something & Misc & Expresses_an_Opinion (Bagging Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Governance\n",
      "==================================================\n",
      "Training Accuracy: 0.9983\n",
      "Test Accuracy: 0.9214\n",
      "Kappa: -0.0362\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96       706\n",
      "          1       0.00      0.00      0.00        19\n",
      "\n",
      "avg / total       0.95      0.92      0.93       725\n",
      "\n",
      "\n",
      "\n",
      "No_Policy_Content\n",
      "==================================================\n",
      "Training Accuracy: 0.9966\n",
      "Test Accuracy: 0.6910\n",
      "Kappa: 0.3686\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.52      0.61       340\n",
      "          1       0.67      0.84      0.74       385\n",
      "\n",
      "avg / total       0.70      0.69      0.68       725\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hng/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asks_for_Donation\n",
      "==================================================\n",
      "Training Accuracy: 0.9997\n",
      "Test Accuracy: 0.9986\n",
      "Kappa: 0.0000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       724\n",
      "          1       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       1.00      1.00      1.00       725\n",
      "\n",
      "\n",
      "\n",
      "Asks_you_to_watch_something_share_something_follow_something\n",
      "==================================================\n",
      "Training Accuracy: 0.9984\n",
      "Test Accuracy: 0.9724\n",
      "Kappa: 0.5117\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99       696\n",
      "          1       0.85      0.38      0.52        29\n",
      "\n",
      "avg / total       0.97      0.97      0.97       725\n",
      "\n",
      "\n",
      "\n",
      "Misc\n",
      "==================================================\n",
      "Training Accuracy: 0.9959\n",
      "Test Accuracy: 0.9779\n",
      "Kappa: 0.3250\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       707\n",
      "          1       0.67      0.22      0.33        18\n",
      "\n",
      "avg / total       0.97      0.98      0.97       725\n",
      "\n",
      "\n",
      "\n",
      "Expresses_an_Opinion\n",
      "==================================================\n",
      "Training Accuracy: 0.9955\n",
      "Test Accuracy: 0.6938\n",
      "Kappa: 0.2284\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.26      0.38       260\n",
      "          1       0.69      0.94      0.80       465\n",
      "\n",
      "avg / total       0.69      0.69      0.65       725\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_samples=0.7\n",
    "max_features=0.8\n",
    "keys = ['Governance', 'No_Policy_Content', 'Asks_for_Donation', \n",
    "        'Asks_you_to_watch_something_share_something_follow_something',\n",
    "        'Misc', 'Expresses_an_Opinion']\n",
    "\n",
    "for key in keys: \n",
    "    # real labels for the training set\n",
    "    tar_train = data[key][0:train_end,] \n",
    "    # real labels for the test set\n",
    "    tar_test = data[key][train_end:,]\n",
    "\n",
    "    # specify the classification model\n",
    "    clf = BaggingClassifier(LinearSVC(class_weight='balanced'),\n",
    "                            max_samples=max_samples, max_features=max_features)\n",
    "    # fit the model with the training set\n",
    "    clf.fit(pred_train, tar_train)\n",
    "    # compute training accuracy\n",
    "    train_score = clf.score(pred_train, tar_train)\n",
    "    # predict labels on the test set\n",
    "    y_pred = clf.predict(pred_test)\n",
    "\n",
    "    # compute standard metrics\n",
    "    test_accuracy = metrics.accuracy_score(tar_test.values.astype(int), y_pred)\n",
    "    class_report = metrics.classification_report(tar_test.values.astype(int), y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(tar_test.values.astype(int), y_pred)\n",
    "\n",
    "    print(key)\n",
    "    print('='*50)\n",
    "    print('Training Accuracy: '+'{:.4f}'.format(train_score))\n",
    "    print('Test Accuracy: '+'{:.4f}'.format(test_accuracy))\n",
    "    print('Kappa: '+'{:.4f}'.format(kappa))\n",
    "    print(class_report)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Four: Final Classifications\n",
    "Having the appropriate models and hyper-parameter settings, we can use the models to predict labels for the entire corpus of tweets. At this step, we take all 7525 human-coded-tweets as the training set, and the whole corpus as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load training data and convert to a vector\n",
    "train_data = pd.DataFrame.from_csv('training_data.csv')\n",
    "train_text = train_data['text'].astype('U')\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize,ngram_range=(1,3))\n",
    "pred_train = vectorizer.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the full corpus and convert to a vector\n",
    "final_data = pd.DataFrame.from_csv('ProcessedFullCorpus.csv')\n",
    "final_text = final_data['text']\n",
    "pred_final = vectorizer.transform(final_text.values.astype('unicode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify classification models and hyper-parameter settings\n",
    "alpha = 0.1\n",
    "max_samples=0.7\n",
    "max_features=0.8\n",
    "\n",
    "def runClassifiers(classifier, pred_train, tar_train, pred_final):\n",
    "    if classifier=='NB':\n",
    "        clf = MultinomialNB(alpha=alpha, fit_prior=True, class_prior=None)\n",
    "    elif classifier=='SVC':\n",
    "        clf = LinearSVC(class_weight='balanced')\n",
    "    elif classifier=='Bag':\n",
    "        clf = BaggingClassifier(LinearSVC(class_weight='balanced'),\n",
    "                                max_samples=0.7, max_features=0.8)\n",
    "    clf.fit(pred_train,tar_train.values.astype(int))\n",
    "    print('fit successfully')\n",
    "    y_class_final = clf.predict(pred_final)\n",
    "    print('predict successfully')\n",
    "    return y_class_final\n",
    "\n",
    "# A list hardcoded to specify the classifier to use for each category\n",
    "classifiers = ['NB','SVC','Bag','Bag','Bag','Bag','Bag','Bag','Bag',\n",
    "               'Bag','Bag','Bag','Bag','Bag','Bag','Bag','SVC','Bag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain column names\n",
    "keys = [key for key in train_data if key != 'text']\n",
    "pred_matrix = np.zeros((len(final_data),18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit successfully\n",
      "predict successfully\n",
      "Sentiment Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Political Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Ideology Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Immigration Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Macroeconomic Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "National_Security Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Crime Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Civil_Rights Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Environment Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Education Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Health_Care Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Governance Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "No_Policy_Content Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Asks_for_Donation Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Asks_you_to_watch_something_share_something_follow_something Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Misc Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Makes_a_Factual_or_Verifiable_Claim Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Expresses_an_Opinion Classifications Done\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# final classifications: save prediction results in a matrix\n",
    "for n, key in enumerate(keys):\n",
    "    tar_train = train_data[key]\n",
    "    y_final = runClassifiers(classifiers[n], pred_train, tar_train, pred_final)\n",
    "    print(key+' Classifications Done')\n",
    "    print('-'*50)\n",
    "    pred_matrix[:,n] = y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert prediction matrix into dataframe\n",
    "final_df = pd.DataFrame(pred_matrix).astype(int)\n",
    "final_df.columns = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Political</th>\n",
       "      <th>Ideology</th>\n",
       "      <th>Immigration</th>\n",
       "      <th>Macroeconomic</th>\n",
       "      <th>National_Security</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Civil_Rights</th>\n",
       "      <th>Environment</th>\n",
       "      <th>Education</th>\n",
       "      <th>Health_Care</th>\n",
       "      <th>Governance</th>\n",
       "      <th>No_Policy_Content</th>\n",
       "      <th>Asks_for_Donation</th>\n",
       "      <th>Asks_you_to_watch_something_share_something_follow_something</th>\n",
       "      <th>Misc</th>\n",
       "      <th>Makes_a_Factual_or_Verifiable_Claim</th>\n",
       "      <th>Expresses_an_Opinion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7251</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3348</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16388</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment  Political  Ideology  Immigration  Macroeconomic  \\\n",
       "12810          0          1         0            0              0   \n",
       "2856           1          1         0            0              0   \n",
       "7251           1          1        -1            0              0   \n",
       "3348           1          1         0            0              0   \n",
       "16388          1          1         0            0              0   \n",
       "\n",
       "       National_Security  Crime  Civil_Rights  Environment  Education  \\\n",
       "12810                  0      0             0            0          0   \n",
       "2856                   0      0             0            0          0   \n",
       "7251                   0      0             0            0          0   \n",
       "3348                   0      0             0            0          0   \n",
       "16388                  0      0             0            0          0   \n",
       "\n",
       "       Health_Care  Governance  No_Policy_Content  Asks_for_Donation  \\\n",
       "12810            0           0                  0                  0   \n",
       "2856             0           0                  1                  0   \n",
       "7251             0           0                  1                  0   \n",
       "3348             0           0                  0                  0   \n",
       "16388            0           0                  1                  0   \n",
       "\n",
       "       Asks_you_to_watch_something_share_something_follow_something  Misc  \\\n",
       "12810                                                  0                0   \n",
       "2856                                                   0                1   \n",
       "7251                                                   0                0   \n",
       "3348                                                   0                0   \n",
       "16388                                                  0                1   \n",
       "\n",
       "       Makes_a_Factual_or_Verifiable_Claim  Expresses_an_Opinion  \n",
       "12810                                    0                     1  \n",
       "2856                                     0                     1  \n",
       "7251                                     0                     1  \n",
       "3348                                     0                     1  \n",
       "16388                                    0                     1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "final_file = pd.concat([final_data.reset_index(drop=True), final_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save prediction result to csv\n",
    "out_file = 'finalPredictionTest.csv'\n",
    "final_file.to_csv(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X',\n",
       " 'id',\n",
       " 'created_at',\n",
       " 'text',\n",
       " 'hashtag.',\n",
       " 'at.',\n",
       " 'link',\n",
       " 'retweets',\n",
       " 'favorites',\n",
       " 'full.URL',\n",
       " 'Name',\n",
       " 'Sentiment',\n",
       " 'Political',\n",
       " 'Ideology',\n",
       " 'Immigration',\n",
       " 'Macroeconomic',\n",
       " 'National_Security',\n",
       " 'Crime',\n",
       " 'Civil_Rights',\n",
       " 'Environment',\n",
       " 'Education',\n",
       " 'Health_Care',\n",
       " 'Governance',\n",
       " 'No_Policy_Content',\n",
       " 'Asks_for_Donation',\n",
       " 'Asks_you_to_watch_something_share_something_follow_something',\n",
       " 'Misc',\n",
       " 'Makes_a_Factual_or_Verifiable_Claim',\n",
       " 'Expresses_an_Opinion']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(final_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
